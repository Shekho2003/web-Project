import time, re, os
from collections import Counter
from urllib.parse import urlparse, parse_qs
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import pandas as pd
from tqdm import tqdm
from textblob import TextBlob
from langdetect import detect

# =============================================================================
## 1. AYARLAR VE KELÄ°ME SÃ–ZLÃœKLERÄ° (Sinema ve Ä°zleyici Segmentasyonu Ä°Ã‡Ä°N)
# =============================================================================

AUDIENCE_SEGMENTS = {
    "Sinefil / Hikaye OdaklÄ±": [
        "senaryo", "hikaye", "kurgu", "plot", "story", "script", "writing", "karakter", 
        "derinlik", "felsefe", "mesaj", "anlatÄ±m", "dialogue", "diyalog", "twist", "sonu", 
        "final", "mantÄ±k", "boÅŸluk", "cliche", "kliÅŸe"
    ],
    "GÃ¶rsel / Aksiyon Sever": [
        "efekt", "cgi", "vfx", "gÃ¶rsel", "visual", "aksiyon", "action", "sahne", "kavga", 
        "patlama", "fight", "renk", "sinematografi", "Ã§ekim", "kamera", "atmosfer", "3d", "imax", 
        "ses", "soundtrack", "mÃ¼zik"
    ],
    "Oyuncu / Fan Kitlesi": [
        "oyuncu", "aktÃ¶r", "aktris", "cast", "acting", "oyunculuk", "performans", "yakÄ±ÅŸÄ±klÄ±", 
        "gÃ¼zel", "kral", "queen", "fan", "hayran", "abi", "abla", "role", "rol", "karizma"
    ],
    "Genel Ä°zleyici (Hype/Tepki)": [
        "hype", "bekliyorum", "heyecan", "sÄ±kÄ±cÄ±", "boring", "zaman", "vakit", "bilet", 
        "sinema", "film", "movie", "izledim", "izlenir", "tavsiye", "fragman", "trailer", "Ã§Ã¶p", "efsane"
    ]
}

SENTIMENT_OVERRIDE = {
    "POSITIVE": ["masterpiece", "baÅŸyapÄ±t", "efsane", "mÃ¼kemmel", "harika", "best", "oscar", "bÃ¼yÃ¼leyici", "soluksuz", "bayÄ±ldÄ±m", "ÅŸahane", "10/10", "hype", "bekliyorum"],
    "NEGATIVE": ["berbat", "Ã§Ã¶p", "trash", "worst", "hayal kÄ±rÄ±klÄ±ÄŸÄ±", "disappoint", "sÄ±kÄ±cÄ±", "boÅŸa", "uyumuÅŸum", "felaket", "rezalet", "cringe", "zaman kaybÄ±"]
}

# Analiz dÄ±ÅŸÄ± bÄ±rakÄ±lacak kelimeler (Stopwords)
STOPWORDS = set(['bir', 've', 'bu', 'da', 'de', 'Ã§ok', 'ama', 'iÃ§in', 'ben', 'sen', 'o', 'the', 'and', 'is', 'to', 'in', 'of', 'it', 'film', 'movie'])

# =============================================================================
# 2. YARDIMCI FONKSÄ°YONLAR
# =============================================================================

def get_video_id(url):
    if not url: return None
    m = re.search(r'(?:youtu\.be/|v=|/v/|embed/)([A-Za-z0-9_-]{11})', url)
    return m.group(1) if m else parse_qs(urlparse(url).query).get('v', [None])[0]

def preprocess_text(text):
    """Metni temizler."""
    if not isinstance(text, str): return ""
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+', '', text)
    text = text.replace('\n', ' ').replace('\r', ' ')
    # Sadece harf ve sayÄ±larÄ± bÄ±rak (Daha temiz kelime analizi iÃ§in)
    # text = re.sub(r'[^\w\s]', '', text) 
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def analyze_cinema_row(text):
    clean_text = preprocess_text(text)
    
    # BoÅŸ veri kontrolÃ¼
    if not clean_text or len(clean_text) < 2:
        return clean_text, "Bilinmiyor", "Neutral", 0.0, "Belirsiz"

    # --- 1. Dil Tespiti ---
    lang = "Bilinmiyor"
    try: 
        if len(clean_text) > 3: lang = detect(clean_text).upper()
    except: pass
    
    # --- 2. Duygu Analizi ---
    blob = TextBlob(clean_text)
    score = blob.sentiment.polarity
    text_lower = clean_text.lower()
    
    s_class = "Neutral"
    # Override KurallarÄ±
    if any(w in text_lower for w in SENTIMENT_OVERRIDE["POSITIVE"]):
        s_class = "Positive"; score = 0.9
    elif any(w in text_lower for w in SENTIMENT_OVERRIDE["NEGATIVE"]):
        s_class = "Negative"; score = -0.9
    else:
        if score > 0.05: s_class = "Positive"
        elif score < -0.05: s_class = "Negative"

    # --- 3. Ä°zleyici Segmentasyonu ---
    detected_segments = []
    for segment, keywords in AUDIENCE_SEGMENTS.items():
        if any(k in text_lower for k in keywords):
            detected_segments.append(segment)
    
    segment_str = ", ".join(detected_segments) if detected_segments else "Belirsiz / Sessiz Ä°zleyici"
    
    return clean_text, lang, s_class, score, segment_str

# =============================================================================
# 3. VERÄ° Ã‡EKME MOTORU
# =============================================================================

def add_comment_to_list(comments_list, seen_set, cid, snippet, parent_id=None):
    if cid in seen_set: return False
    raw_text = snippet.get('textDisplay', "")
    if not raw_text: return False

    clean, lang, s_class, s_score, segment = analyze_cinema_row(raw_text)

    # Ã‡ok kÄ±sa veya boÅŸ yorumlarÄ± listeye ekleme (Veri Ã–n Ä°ÅŸleme)
    if not clean: return False

    row = {
        'ID': cid,
        'Tip': "YanÄ±t" if parent_id else "Ana Yorum",
        'Yazar': snippet.get('authorDisplayName'),
        'Orjinal_Yorum': raw_text,
        'Temizlenmis_Yorum': clean,
        'Begeni': int(snippet.get('likeCount', 0)),
        'Tarih': snippet.get('publishedAt'),
        'Dil': lang,
        'Duygu_Sinifi': s_class,
        'Duygu_Skoru': round(s_score, 2),
        'Izleyici_Segmenti': segment
    }
    comments_list.append(row)
    seen_set.add(cid)
    return True

def fetch_replies_deep(yt, parent_id, seen_set, comments_list, max_limit):
    token = None
    while len(comments_list) < max_limit:
        try:
            req = yt.comments().list(
                part="snippet", parentId=parent_id, maxResults=100, pageToken=token, textFormat="plainText"
            )
            res = req.execute()
            if not res.get('items'): break
            for item in res['items']:
                if len(comments_list) >= max_limit: return
                add_comment_to_list(comments_list, seen_set, item['id'], item['snippet'], parent_id)
            token = res.get('nextPageToken')
            if not token: break
            time.sleep(0.05)
        except: break

def master_fetch(api_key, video_id, target_count):
    try:
        yt = build('youtube', 'v3', developerKey=api_key)
    except Exception as e:
        print(f"\nâŒ API BaÄŸlantÄ± HatasÄ±: {e}"); return []

    all_comments = []
    seen_ids = set()
    print(f"\nğŸš€ SÄ°NEMA VERÄ°SÄ° Ã‡EKÄ°LÄ°YOR... Hedef: {target_count}")
    
    for order in ['relevance', 'time']:
        if len(all_comments) >= target_count: break
        token = None
        pbar = tqdm(total=target_count, unit=" yorum", initial=len(all_comments), desc=f"Mod: {order.upper()}")
        while len(all_comments) < target_count:
            try:
                req = yt.commentThreads().list(
                    part="snippet,replies", videoId=video_id, maxResults=100, 
                    pageToken=token, textFormat="plainText", order=order
                )
                res = req.execute()
                if not res.get('items'): break
                for item in res['items']:
                    if len(all_comments) >= target_count: break
                    top_snip = item['snippet']['topLevelComment']['snippet']
                    top_id = item['id']
                    if add_comment_to_list(all_comments, seen_ids, top_id, top_snip):
                        pbar.update(1)
                    reply_count = item['snippet']['totalReplyCount']
                    if reply_count > 0 and 'replies' in item:
                        for rep in item['replies']['comments']:
                            if len(all_comments) >= target_count: break
                            add_comment_to_list(all_comments, seen_ids, rep['id'], rep['snippet'], top_id)
                            pbar.update(1)
                        if reply_count > len(item.get('replies', {}).get('comments', [])):
                            fetch_replies_deep(yt, top_id, seen_ids, all_comments, target_count)
                            pbar.n = len(all_comments); pbar.refresh()
                token = res.get('nextPageToken')
                if not token: break
            except HttpError as e:
                if "quotaExceeded" in str(e): print("\nâš ï¸ API KOTASI DOLDU!"); return all_comments
                if e.resp.status == 403: return all_comments
                time.sleep(1)
            except Exception as e:
                print(f"Hata: {e}"); time.sleep(1)
        pbar.close()
    return all_comments

# =============================================================================
# 4. DETAYLI Ä°Ã‡ERÄ°K ANALÄ°ZÄ° FONKSÄ°YONLARI
# =============================================================================

def analyze_content_details(df_subset, label):
    """
    Belirli bir veri kÃ¼mesi (Ã–rn: Sadece Olumlular) iÃ§in detaylÄ± analiz yapar.
    """
    if df_subset.empty:
        print(f"   âš ï¸ {label} iÃ§in yeterli veri yok.")
        return

    # 1. En Ã‡ok KonuÅŸulan Konular (Segmentler Ã¼zerinden)
    segments = df_subset['Izleyici_Segmenti'].str.split(', ').explode()
    # "Belirsiz" olanlarÄ± Ã§Ä±kar, Ã§Ã¼nkÃ¼ konuyu anlamaya Ã§alÄ±ÅŸÄ±yoruz
    segments = segments[segments != "Belirsiz / Sessiz Ä°zleyici"]
    
    print(f"\n   ğŸ“Œ {label} NE HAKKINDA KONUÅUYOR? (Top 3 Konu)")
    if not segments.empty:
        for seg, count in segments.value_counts().head(3).items():
            print(f"      ğŸ”¹ {seg}: {count} kez")
    else:
        print("      (Konu tespiti yapÄ±lamadÄ±)")

    # 2. En SÄ±k KullanÄ±lan Kelimeler (Word Frequency)
    text_blob = " ".join(df_subset['Temizlenmis_Yorum'].astype(str)).lower()
    # Noktalama iÅŸaretlerini kaldÄ±r
    text_blob = re.sub(r'[^\w\s]', '', text_blob)
    words = text_blob.split()
    # Stopwords temizliÄŸi
    clean_words = [w for w in words if w not in STOPWORDS and len(w) > 2]
    
    most_common = Counter(clean_words).most_common(5)
    
    print(f"\n   ğŸ’¬ {label} EN Ã‡OK HANGÄ° KELÄ°MELERÄ° KULLANIYOR?")
    if most_common:
        print(f"      ğŸ‘‰ {', '.join([f'{w[0]} ({w[1]})' for w in most_common])}")
    else:
        print("      (Yeterli kelime verisi yok)")

# =============================================================================
# 5. RAPORLAMA
# =============================================================================

def generate_cinema_report(df):
    print("\n" + "="*60)
    print("ğŸ¬ SÄ°NEMA & Ä°ZLEYÄ°CÄ° SEGMENTASYON RAPORU (V2.0)")
    print("="*60)
    
    total = len(df)
    if total == 0: print("Veri yok."); return

    print(f"\nğŸ‘¥ Toplam Ä°ÅŸlenen Yorum: {total}")

    # --- DUYGU ORANLARI ---
    pos_df = df[df['Duygu_Sinifi'] == 'Positive']
    neg_df = df[df['Duygu_Sinifi'] == 'Negative']
    
    print("\nğŸ“Š GENEL DUYGU DURUMU:")
    print(f"   ğŸ¿ Olumlu (BeÄŸeni): {len(pos_df)} (%{len(pos_df)/total*100:.1f})")
    print(f"   ğŸ… Olumsuz (EleÅŸtiri): {len(neg_df)} (%{len(neg_df)/total*100:.1f})")

    # --- DETAYLI POZÄ°TÄ°F ANALÄ°Z ---
    print("\n" + "-"*40)
    print("ğŸŸ¢ POZÄ°TÄ°F YORUMLARIN Ä°Ã‡ERÄ°K ANALÄ°ZÄ°")
    print("-" * 40)
    analyze_content_details(pos_df, "BEÄENENLER")

    # --- DETAYLI NEGATÄ°F ANALÄ°Z ---
    print("\n" + "-"*40)
    print("ğŸ”´ NEGATÄ°F YORUMLARIN Ä°Ã‡ERÄ°K ANALÄ°ZÄ°")
    print("-" * 40)
    analyze_content_details(neg_df, "ELEÅTÄ°RENLER")

    print("\n" + "="*60)

# =============================================================================
# 6. MAIN
# =============================================================================

def main():
    print("ğŸ¥ SÄ°NEMA ANALÄ°Z ARACI (V2.0) BAÅLATILIYOR...")
    api_key = input("ğŸ”‘ API KEY: ").strip()
    url = input("ğŸ”— Video URL: ").strip()
    try: target = int(input("ğŸ”¢ Hedef (Enter=2000): ") or 2000)
    except: target = 2000
    
    vid = get_video_id(url)
    if not vid: print("âŒ HatalÄ± Link"); return

    data = master_fetch(api_key, vid, target)
    
    if data:
        df = pd.DataFrame(data)
        fname = f"Sinema_Analiz_{int(time.time())}.xlsx"
        df.to_excel(fname, index=False)
        print(f"\nâœ… Veriler kaydedildi: {fname}")
        generate_cinema_report(df)
    else:
        print("\nâŒ Veri Ã§ekilemedi.")

if __name__ == "__main__":
    main()